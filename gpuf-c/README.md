# GPUFabric Client (gpuf-c)

GPUFabric client supporting distributed inference with multiple LLM engines.

## ğŸš€ Quick Start

### Build
```powershell
cargo build --release
```

### Standalone LLAMA Mode
```powershell
.\target\release\gpuf-c.exe --standalone-llama
```

### Worker Mode
```powershell
.\target\release\gpuf-c.exe `
    --engine-type llama `
    --llama-model-path ./model.gguf `
    --server-addr 192.168.1.100
```

## ğŸ“ Project Structure

```
gpuf-c/
â”œâ”€â”€ src/           # Source code
â”œâ”€â”€ docs/          # Documentation
â”œâ”€â”€ scripts/       # Build scripts
â”œâ”€â”€ tests/         # Test scripts
â”œâ”€â”€ examples/      # Example code
â””â”€â”€ jniLibs/       # Android libraries
```

## ğŸ“– Documentation

- [Windows Build Guide](docs/WINDOWS_BUILD.md) - Build instructions for Windows

## ğŸ§ª Testing

```powershell
# Run LLAMA tests
.\tests\test_llama_worker.ps1

# Run API tests
.\tests\test_api.ps1

# Run Vulkan tests
.\tests\test_vulkan.ps1
```

## ğŸ”§ Supported Engines

- **llama.cpp** - High-performance local inference
- **Ollama** - Containerized LLM service
- **VLLM** - High-performance inference service

## ğŸ¯ Features

- âœ… Standalone and cluster modes
- âœ… OpenAI compatible API
- âœ… GPU acceleration (Vulkan/CUDA)
- âœ… Automatic model download
- âœ… Cross-platform support

## ğŸ¤ Contributing

Issues and Pull Requests are welcome!

## ğŸ“„ License

[MIT License](LICENSE)
